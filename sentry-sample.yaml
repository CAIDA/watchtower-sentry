# Watchtower-Sentry configuration file
#
# This is a worked example using the Trinarkular Per-Country detection instance


# Global loglevel can be DEBUG, INFO (default), WARNING, ERROR, or CRITICAL
loglevel: INFO


# Pipeline is a list of modules that are chained together, passing a stream of
# (key, value, time) tuples from one to the next.  A pipeline starts with a
# source, followed by any number of filters, and ends with a sink.  All
# modules have a "module" attribute identifying the module, and an optional
# "loglevel" attribute that overrides the global loglevel within the module.
pipeline:

# Start with a source module.  A real config file can have only one source;
# multiple are shown here for illustration only.

# Obtain time series data from TSK (Time Series Kafka) in real-time
- module: "sources.Realtime"

  # Fetch data whose keys match this DBATS-style glob pattern
  expression: 'active.ping-slash24.geo.netacuity.*.*.probers.team-*.caida-sdsc.*.up_slash24_cnt'

  # Comma-separated list of Kafka brokers in <host>:<port> form.
  brokers: 'clayface.caida.org:9092,croc.caida.org:9092,loki.caida.org:9092'

  # Kafka topic prefix
  topicprefix: 'tsk-production'

  # Kafka topic channel (appended to topic prefix to create a topic name)
  channelname: 'active.ping-slash24.team-1.aggregated'

  # Kafka consumer group
  consumergroup: 'test'


# Obtain time series data by querying the IODA HTTP API
- module: "sources.Historical"

  # Fetch data whose keys match this DBATS-style glob pattern
  expression: 'active.ping-slash24.geo.netacuity.*.*.probers.team-*.caida-sdsc.*.up_slash24_cnt'

  # Fetch data with time >= starttime and time < endtime
  starttime: '2019-01-01 00:00'
  endtime: '2019-07-01 00:00'

  # IODA HTTP API URL
  url: 'https://ioda.caida.org/data/ts/json'

  # How many seconds of data should be retrieved with each API call.
  batchduration: 21600

  # If true, null values will be skipped. If false, they will be treated as 0.
  # (optional, default false)
  ignorenull: true

  # (optional) Additional query parameters to pass to the API.
  # In this case the aggrScheme setting disables any "software" downsampling,
  # and sets a max points value that will force DBATS to give us full-
  # resolution data (since we're querying in 6hr batches, there should only be
  # 36 points at full 10min resolution).
  queryparams:
    aggrScheme: 'db'
    maxPointsPerSeries: 100


# Read time series data from a JSONL file.  This is mainly useful for testing.
- module: "sources.JsonIn"

  # Filename to read jsonl records from. "-" (the default) means stdin.
  file: "in.jsonl"


# Convert unsigned 64-bit numbers to signed 64-bit numbers.
- module: "filters.ToSigned"


# Aggregate sum across multiple keys with same timestamp
- module: "filters.AggSum"

  # DBATS-style glob pattern used to group series to be aggregated.    One or
  # more parenthesized substrings identify the group over which to aggregate.
  # The output key will be expression with parenthesized substrings replaced
  # with the actual group substrings seen in the input keys.
  # Example:
  #   input keys: x.US.p1, x.US.p2, x.CA.p1, x.CA.p2
  #   expression: x.(*).*
  #   output keys: x.US.* x.CA.*
  expression: 'active.ping-slash24.geo.netacuity.(*.*).probers.team-*.caida-sdsc.*.up_slash24_cnt'

  # (optional) Expected number of inputs per group.  Once a group has this
  # many values, the output can be generated, even if timeout has not been
  # reached.
  groupsize: 20

  # Max time (in seconds) to wait for inputs to arrive for a group before
  # generating output for the group (unless droppartial is set).
  timeout: 600

  # (optional, default False) If this is true, then groups with fewer than
  # {groupsize} datapoints after {timeout} seconds should be dropped rather
  # than generating output.
  droppartial: false


# Output ratio of actual values to predicted values calculated with a moving
# statistic.
- module: "filters.MovingStat"

  # How to calculate the predicted value.  This is an array, starting with the
  # name of the method, possibly followed by additional integer parameters.
  type: ['median']             # middle value; equivalent to ['quantile', 1, 2]
  #type: ['min']                # min value; equivalent to ['quantile', 0, 1]
  #type: ['max']                # max value; equivalent to ['quantile', 1, 1]
  #type: ['mean']               # mean of values
  #type: ['quantile', 1, 4]     # 1st quartile
  #type: ['quantile', 90, 100]  # 90th percentile

  # Number of seconds of data over which to calculate statistic.
  history: 604800

  # Minimum number of seconds of data to collect before generating output.
  warmup: 3600

  # (optional) To avoid having extreme values skew the prediction, we can
  # "inpaint": That is, instead of appending an actual extreme value to the
  # history, we instead append the predicted value.
  inpainting:
    # Inpaint if actual/predicted is < min or > max.  At least one of min or
    # max is required.
    min: 0.8
    max: 2

    # We occasionally see level shifts in the signals that can result in
    # perpetual alerts if we don't update the history because we're in a
    # permanent inpainting state. This should help mitigate this by only
    # inpainting for a certain amount of time, after which old data is
    # discarded and the new (apparently normal) data is added to the history.
    #
    # This effectively limits the maximum outage duration that we
    # can detect, but since level shifts and outages are identical
    # as far as our data goes, I don't think there is a better
    # solution.
    maxduration: 3600


# End with a sink module.  A real config file can have only one sink;
# multiple are shown here for illustration only.

# Detect extreme values and send alert objects to a Kafka cluster.
- module: "sinks.AlertKafka"

  # Generate alert if value < min or value > max.  At least one of min or max
  # is required.
  min: 0.8

  # Unique identifier for this data source (this is constant across
  # aggregation levels). Used when constructing an Alert object.
  fqid: active.ping-slash24.team-1.up_slash24_cnt

  # Human-readable name for this data source (constant across
  # aggregation levels).
  name: '/24 Ping up /24s (Team 1)'

  # Kafka configuration for sending alert data to watchtower-alert system
  brokers: 'watchtower.int.limbo.caida.org:9092'
  topicprefix: 'watchtower-test'


# Write time series data to a JSONL file.  This is mainly useful for testing.
- module: "sinks.JsonOut"

  # Filename to write jsonl records to. "-" (the default) means stdout.
  file: "out.jsonl"
